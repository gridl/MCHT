% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/MonteCarloTest.R
\name{MCHTest}
\alias{MCHTest}
\title{Create an MCHTest Object}
\usage{
MCHTest(test_stat, stat_gen, rand_gen = runif, N = 10000,
  seed = NULL, memoise_sample = TRUE, pval_func = MCHT::pval,
  method = "Monte Carlo Test", test_params = NULL,
  fixed_params = NULL, nuisance_params = NULL, optim_control = NULL,
  tiebreaking = FALSE, lock_alternative = TRUE, threshold_pval = 1,
  suppress_threshold_warning = FALSE)
}
\arguments{
\item{test_stat}{A function that computes the test statistic from input data;
\code{x} must be a parameter of this function representing
test data}

\item{stat_gen}{A function that generates values of the test statistic when
given data; \code{x} (representing a sample) must be a
parameter of this function, and this function is expected to
return one numeric output, but if \code{n} is a parameter,
this will be interpreted as sample size information (this
could be useful for allowing a "burn-in" period in random
data, as is often the case when working with time series
data)}

\item{rand_gen}{A function generating random data, accepting a parameter
\code{n} (representing the size of the data) or \code{x}
(which would be the actual data)}

\item{N}{Integer representing the number of replications of \code{stat_gen}
to generate}

\item{seed}{The random seed used to generate simulated statistic values; if
\code{NULL}, the seed will be randomly chosen each time the
resulting function is called (unless \code{memoise_sample} is
\code{TRUE})}

\item{memoise_sample}{If \code{TRUE}, simulated statistic values are saved
and will be used repeatedly if the inputs to
\code{stat_gen} don't change (such as the sample size,
\code{n}); this could be in conflict with \code{seed}
if \code{seed} is \code{NULL}, so set to \code{FALSE}
to allow for regeneration of random samples for every
call to the resulting function}

\item{pval_func}{A function that computes \eqn{p}-values from the test
statistic computed by \code{test_stat} using the simulated
data generated via \code{stat_gen}; see \code{\link{pval}}
for an example of how this function should be specified}

\item{method}{A string labelling the test}

\item{test_params}{A character vector of the names of parameters with values
specified under the null hypothesis; both \code{test_stat}
and \code{stat_gen} need to be able to recognize the
contents of this vector as parameters (for example, if
this argument is \code{"mu"}, then \code{mu} needs to be
an argument of both \code{test_stat} and \code{stat_gen}),
and the resulting test will try to pass these parameters
to \code{rand_gen} (but these \emph{do not} need to be
parameters of \code{rand_gen})}

\item{fixed_params}{A character vector of the names of parameters treated as
fixed values; this isn't needed but if these parameters
are being used then test output is more informative and
errors will be raised if \code{test_stat} and
\code{stat_gen} don't accept these parameters—which is
safer—and the resulting test will try to pass these
parameters to \code{rand_gen} (but these \emph{do not}
need to be parameters of \code{rand_gen})}

\item{nuisance_params}{A character vector of the names of parameters to be
treated as nuisance parameters which must be chosen
via optimization (see \insertCite{dufour06}{MCHT});
must be parameters of \code{test_stat} and
\code{stat_gen}, but these \emph{will not} be viewed
as parameters of \code{rand_gen}, and cannot be
non-\code{NULL} if code{optim_control} is \code{NULL}}

\item{optim_control}{A list of arguments to be passed to
\code{\link[GenSA]{GenSA}}, containing at least
\code{lower} and \code{upper} elements as named vectors,
with the names being identical to
\code{nuisance_params}, but could also include other
arguments to be passed to \code{\link[GenSA]{GenSA}};
the \code{fn} parameter will be set, and parameters of
that function will be the parameters mentioned in
\code{nuisance_params}, and this argument will be
ignored if \code{nuisance_params} is \code{NULL}}

\item{tiebreaking}{Break ties using the method as described in
\insertCite{dufour06;textual}{MCHT}; won't work if
\code{pval_func} doesn't support it via a \code{unif_gen}
argument, and should only be used for test statistics not
computed on continuously-distributed data}

\item{lock_alternative}{If \code{TRUE}, then the resulting function will
effectively ignore the \code{alternative} parameter,
while if \code{FALSE}, the resulting function will be
sensitive to values of \code{alternative}; this
argument exists to prevent shooting yourself in the
foot and accidentally computing \eqn{p}-values in
inappropriate ways}

\item{threshold_pval}{A numeric value that represents a threshold
\eqn{p}-value that, if surpassed by the optimization
algorithm, will cause the algorithm to terminate; will
override the \code{threshold.stop} argument in the
\code{control} list that's used by
\code{\link[GenSA]{GenSA}}}

\item{suppress_threshold_warning}{If \code{TRUE}, user will not be warned if
the threshold \eqn{p}-value was surpassed
by the optimization algorithm}
}
\value{
A \code{MCHTest}-class object, a function with parameters \code{x},
        \code{alternative}, and \code{...}, with other parameters being
        passed to functions such as those passed to \code{test_stat} and
        \code{stat_gen}, controlling what's tested and how; depending on
        \code{lock_alternative}, the \code{alternative} argument may be
        ignored
}
\description{
This function creates an \code{MCHTest}-class object, an S3 object that
defines a bootstrap or Monte Carlo test.
}
\details{
\code{MCHTest}-class objects are effectively functions that accept data and
maybe some parameters and return an \code{htest}-class object containing the
results of a Monte Carlo or bootstrap statistical test. These object will
accept datasets and perhaps some parameters and will return the results of a
test.

Bootstrap tests can be implemented when the dataset is passed as an argument
to \code{rand_gen} (which occurs when \code{x} is one of \code{rand_gen}'s
parameters). The only difference between a Monte Carlo test and a bootstrap
test in the context of this function is that bootstrap tests use information
from the original dataset when generating simulated test statistics, while a
Monte Carlo test does not. When the default function for computing
\eqn{p}-values is used, this function will perform a test similar to that
described by \insertCite{mackinnon09;textual}{MCHT}.

For Monte Carlo tests, when the default function for computing \eqn{p}-values
is used (see \code{\link{pval}}), this is effectively the test described in
\insertCite{dufour06;textual}{MCHT}. This includes using simulated annealing
to find values of nuisance parameters that maximize the \eqn{p}-value if the
null hypothesis is true. Simulated annealing is implemented using
\code{\link[GenSA]{GenSA}} from the \pkg{GenSA} package, and the
\code{optim_control} parameter is used for controlling \code{GenSA}'s
behavior. We highly recommend reading \code{GenSA}'s documentation.

The \code{threshold_pval} argument can be used for stopping the optimization
procedure when a specified \eqn{p}-value is reached or surpassed.
\insertCite{dufour06;textual}{MCHT} showed that \eqn{p}-values found using
the procedure implemented here are conservative (in the sense that they are
larger than they necessarily need to be). If the algorithm terminates early
due to surpassing a prespecified \eqn{p}-value, then the estimated
\eqn{p}-value is known to at least be the value returned, but because the
\eqn{p}-value is a conservative estimate of the "true" \eqn{p}-value, this
latter number could be smaller. Thus we cannot say much about the location of
the true \eqn{p}-value if the algorithm terminates early. For this reason, a
\code{MCHTest}-class function will, by default, issue a warning if the
algorithm terminated early. However, by setting
\code{suppress_threshold_warning} to \code{TRUE}, this behavior can be
disabled. This recognizes the fact that even though an early termination
leads to us not being able to say much about the location of the true
\eqn{p}-value, we know that whatever the more accurate estimate is, we would
not reject the null hypothesis based on that result.
}
\examples{
dat <- c(0.16, 1.00, 0.67, 1.28, 0.31, 1.16, 1.25, 0.93, 0.66, 0.54)
# Monte Carlo t-test for exponentially distributed data
mc.t.test <- MCHTest(test_stat = function(x, mu = 1) {
                       sqrt(length(x)) * (mean(x) - mu)/sd(x)
                     }, stat_gen = function(x, mu = 1) {
                       x <- x * mu
                       sqrt(length(x)) *  (mean(x) - mu)/sd(x)
                     }, rand_gen = rexp, seed = 123,
                     method = "Monte Carlo t-Test", test_params = "mu",
                     lock_alternative = FALSE)
mc.t.test(dat)
mc.t.test(dat, mu = 0.1, alternative = "two.sided")

# Testing for the scale parameter of a Weibull distribution
# Two-sided test for location of scale parameter
library(fitdistrplus)
ts <- function(x, scale = 1) {
  fit_null <- coef(fitdist(x, "weibull", fix.arg = list("scale" = scale)))
  kt <- fit_null[["shape"]]
  l0 <- scale
  fit_all <- coef(fitdist(x, "weibull"))
  kh <- fit_all[["shape"]]
  lh <- fit_all[["scale"]]
  n <- length(x)

  # Test statistic, based on the negative-log-likelihood ratio
  suppressWarnings(n * ((kt - 1) * log(l0) - (kh - 1) * log(lh) -
      log(kt/kh) - log(lh/l0)) - (kt - kh) * sum(log(x)) + l0^(-kt) *
      sum(x^kt) - lh^(-kh) * sum(x^kh))
}

sg <- function(x, scale = 1, shape = 1) {
  x <- qweibull(x, shape = shape, scale = scale)
  ts(x)
}

mc.wei.shape.test <- MCHTest(ts, sg, seed = 123, test_params = "scale",
                             nuisance_params = "shape",
                             optim_control = list(
                               lower = c("shape" = 0),
                               upper = c("shape" = 100)
                             ), threshold_pval = .2, N = 1000)

mc.wei.shape.test(rweibull(100, shape = 4), shape = 2)

# Bootstrap hypothesis test
# Kolmogorov-Smirnov test for Weibull distribution via parametric botstrap
# hypothesis test

ts <- function(x) {
  param <- coef(fitdist(x, "weibull"))
  shape <- param[['shape']]; scale <- param[['scale']]
  ks.test(x, pweibull, shape = shape, scale = scale,
          alternative = "two.sided")$statistic[[1]]
}

sg <- function(x, delta = 0) {
  ts(x)
}

rg <- function(x) {
  n <- length(x)
  param <- coef(fitdist(x, "weibull"))
  shape <- param[['shape']]; scale <- param[['scale']]
  rweibull(n, shape = shape, scale = scale)
}

b.ks.test <- MCHTest(test_stat = ts, stat_gen = sg, rand_gen = rg,
                     seed = 123, N = 1000)
b.ks.test(rbeta(100, 2, 2))

# Permutation test

df <- data.frame(
  val = c(rnorm(5, mean = 2, sd = 3), rnorm(10, mean = 1, sd = 2)),
  group = rep(c("x", "y"), times = c(5, 10))
)

ts <- function(x, delta = 0) {
  means <- aggregate(val ~ group, data = x, mean)
  vars <- aggregate(val ~ group, data = x, var)
  counts <- aggregate(val ~ group, data = x, length)

  (means$val[1] - means$val[2] - delta)/sum(vars$val / sqrt(counts$val))
}

sg <- function(x, delta = 0) {
  ts(x, delta)
}

rg <- function(x) {
  x$group <- sample(x$group)
  x
}

permute.test <- MCHTest(ts, sg, rg, seed = 123, N = 1000,
                        test_params = "delta",
                        lock_alternative = FALSE)

permute.test(df, alternative = "two.sided")
}
